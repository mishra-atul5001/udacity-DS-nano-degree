{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b261c0",
   "metadata": {},
   "source": [
    "## Here we train our own custom NER model\n",
    "\n",
    "This is something exactly we are doing in main notebook, but this one is reproducible for others!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f2a58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Losses {'ner': 11.28571343421936}\n",
      "Iteration 1: Losses {'ner': 11.028032541275024}\n",
      "Iteration 2: Losses {'ner': 10.733757138252258}\n",
      "Iteration 3: Losses {'ner': 10.449460625648499}\n",
      "Iteration 4: Losses {'ner': 10.11997103691101}\n",
      "Iteration 5: Losses {'ner': 9.6016104221344}\n",
      "Iteration 6: Losses {'ner': 9.049630999565125}\n",
      "Iteration 7: Losses {'ner': 8.3404620885849}\n",
      "Iteration 8: Losses {'ner': 7.712250351905823}\n",
      "Iteration 9: Losses {'ner': 6.88342410326004}\n",
      "Iteration 10: Losses {'ner': 5.032250463962555}\n",
      "Iteration 11: Losses {'ner': 4.616854697465897}\n",
      "Iteration 12: Losses {'ner': 3.442632034420967}\n",
      "Iteration 13: Losses {'ner': 2.650576204061508}\n",
      "Iteration 14: Losses {'ner': 2.4980225265026093}\n",
      "Iteration 15: Losses {'ner': 2.1496297419071198}\n",
      "Iteration 16: Losses {'ner': 2.26289115101099}\n",
      "Iteration 17: Losses {'ner': 2.5032800608314574}\n",
      "Iteration 18: Losses {'ner': 2.8626511560869403}\n",
      "Iteration 19: Losses {'ner': 2.3896844245682587}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the blank English language model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create a new named entity recognizer and add it to the pipeline\n",
    "ner = nlp.add_pipe('ner')\n",
    "\n",
    "# Define the categories of entities you want to recognize\n",
    "labels = ['PERSON', 'ORG', 'GPE']\n",
    "\n",
    "# Load the annotated data into Spacy format\n",
    "TRAIN_DATA = [\n",
    "    ('John Smith is a person', {'entities': [(0, 10, 'PERSON')]}),\n",
    "    ('Acme Inc. is an organization', {'entities': [(0, 8, 'ORG')]}),\n",
    "    ('London is a city', {'entities': [(0, 6, 'GPE')]}),\n",
    "    # more examples...\n",
    "]\n",
    "\n",
    "# Convert the annotated data to Example objects\n",
    "TRAIN_EXAMPLES = []\n",
    "for text, annotations in TRAIN_DATA:\n",
    "    entities = annotations.get('entities')\n",
    "    example = Example.from_dict(nlp.make_doc(text), {'entities': entities})\n",
    "    TRAIN_EXAMPLES.append(example)\n",
    "\n",
    "# Define the training function\n",
    "def train_spacy_ner(nlp, train_data, labels, n_iter=20):\n",
    "    # Get the ner component from the pipeline\n",
    "    ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # Add the labels to the ner component\n",
    "    for label in labels:\n",
    "        ner.add_label(label)\n",
    "\n",
    "    # Disable other pipeline components that don't need to be trained\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        # Initialize the optimizer\n",
    "        optimizer = nlp.begin_training()\n",
    "\n",
    "        # Loop over the training data in batches\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                examples = []\n",
    "                for example in batch:\n",
    "                    examples.append(example)\n",
    "                nlp.update(examples, sgd=optimizer, drop=0.35, losses=losses)\n",
    "\n",
    "            # Print the losses during training\n",
    "            print('Iteration %d: Losses %s' % (itn, losses))\n",
    "\n",
    "# Train the model using the annotated data\n",
    "train_spacy_ner(nlp, TRAIN_EXAMPLES, labels)\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk('custom_ner_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a29d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
